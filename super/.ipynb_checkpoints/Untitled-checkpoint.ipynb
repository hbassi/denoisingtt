{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04cbc651-0b31-48ee-b352-a48f26059a69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/h/hbassi/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8, 1, 192, 192])) that is different to the input size (torch.Size([8, 1, 96, 96])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (96) must match the size of tensor b (192) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionTransformer(image_size\u001b[38;5;241m=\u001b[39mimage_size, patch_size\u001b[38;5;241m=\u001b[39mpatch_size, dim\u001b[38;5;241m=\u001b[39mdim, depth\u001b[38;5;241m=\u001b[39mdepth, heads\u001b[38;5;241m=\u001b[39mheads, mlp_dim\u001b[38;5;241m=\u001b[39mmlp_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[43mtrain_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m    157\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvit_super_resolution_from_scratch.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m, in \u001b[0;36mtrain_vit\u001b[0;34m(model, train_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/nn/functional.py:3365\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/.local/perlmutter/python-3.11/lib/python3.11/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (96) must match the size of tensor b (192) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dataset class to load the fine and coarse scale dynamics (3-index tensors)\n",
    "class DynamicsDataset(Dataset):\n",
    "    def __init__(self, fine_scale_images, coarse_scale_images, transform=None):\n",
    "        self.fine_scale_images = fine_scale_images\n",
    "        self.coarse_scale_images = coarse_scale_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.fine_scale_images.shape[0]  # T: number of time steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fine_img = self.fine_scale_images[idx]  # shape: (192, 192)\n",
    "        coarse_img = self.coarse_scale_images[idx]  # shape: (384, 384)\n",
    "\n",
    "        # Resize the images manually if needed (replace `transform` logic)\n",
    "        if self.transform:\n",
    "            fine_img = self.transform(fine_img)\n",
    "            coarse_img = self.transform(coarse_img)\n",
    "\n",
    "        return fine_img, coarse_img\n",
    "\n",
    "\n",
    "# Manual resizing function (equivalent to torchvision.transforms.Resize)\n",
    "def resize_image(image, size):\n",
    "    return F.interpolate(image.unsqueeze(0), size=size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=192, patch_size=16, dim=512, depth=12, heads=8, mlp_dim=1024):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Patch Embedding: Linear projection of flattened patches\n",
    "        self.patch_embed = nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Positional Encoding (learnable)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, self.num_patches, dim))\n",
    "        \n",
    "        # Transformer Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Upsample Layer\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Final Convolution Layer\n",
    "        self.fc = nn.Conv2d(dim, 1, kernel_size=1)  # Output one channel for the coarse image\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, 192, 192) for fine scale input\n",
    "\n",
    "        # Patch embedding: (batch_size, 1, 192, 192) -> (batch_size, dim, num_patches, 1) -> (batch_size, num_patches, dim)\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Add positional encoding to patches\n",
    "        x = x + self.positional_encoding\n",
    "        \n",
    "        # Pass through the transformer encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Reshape and apply the final convolution\n",
    "        x = x.transpose(1, 2).reshape(x.shape[0], -1, int(math.sqrt(self.num_patches)), int(math.sqrt(self.num_patches)))\n",
    "\n",
    "        # First upsample (scale factor 2) to get (batch_size, 1, 48, 48)\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Second upsample (scale factor 2) to get (batch_size, 1, 96, 96)\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Third upsample (scale factor 2) to get (batch_size, 1, 192, 192)\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Apply the final convolution (single channel output)\n",
    "        x = self.fc(x)  # Final output layer, producing coarse scale prediction\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_vit(model, train_loader, num_epochs=10, lr=1e-4):\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # For regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to device (GPU/CPU)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "image_size = 192  # Input image size\n",
    "patch_size = 16  # Patch size for ViT\n",
    "dim = 512  # Dimensionality of transformer\n",
    "depth = 12  # Number of transformer encoder layers\n",
    "heads = 8  # Number of attention heads\n",
    "mlp_dim = 1024  # Feedforward network dimension\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "\n",
    "# Assuming your fine_scale_images and coarse_scale_images are 3D tensors:\n",
    "# fine_scale_images.shape = (T, 192, 192), coarse_scale_images.shape = (T, 384, 384)\n",
    "# Where T is the number of time steps\n",
    "\n",
    "# Load your data (replace with actual data loading)\n",
    "fine_scale_images = torch.randn(78, 192, 192)  # Placeholder\n",
    "coarse_scale_images = torch.randn(78, 384, 384)  # Placeholder\n",
    "\n",
    "# Reshape to (T, H, W, C) to match dataset input format (batch_size, channels, height, width)\n",
    "fine_scale_images = fine_scale_images.unsqueeze(1)  # shape: (T, 1, 192, 192)\n",
    "coarse_scale_images = coarse_scale_images.unsqueeze(1)  # shape: (T, 1, 384, 384)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = DynamicsDataset(fine_scale_images, coarse_scale_images, transform=lambda x: resize_image(x, (192, 192)))\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionTransformer(image_size=image_size, patch_size=patch_size, dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_vit(model, train_loader, num_epochs=num_epochs, lr=1e-4)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'vit_super_resolution_from_scratch.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d5057-453f-48d0-a094-03606762c29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
